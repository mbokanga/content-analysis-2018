{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Word Embeddings Supplemental\n",
    "\n",
    "This notebook contains two additional uses for word embeddings\n",
    "\n",
    "For this notebook we will be using the following packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "import gensim#For word2vec, etc\n",
    "import requests #For downloading our datasets\n",
    "import nltk #For stop words and stemmers\n",
    "import numpy as np #For arrays\n",
    "import pandas #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "import sklearn.metrics.pairwise #For cosine similarity\n",
    "import sklearn.manifold #For T-SNE\n",
    "import sklearn.decomposition #For PCA\n",
    "import copy\n",
    "\n",
    "#gensim uses a couple of deprecated features\n",
    "#we can't do anything about them so lets ignore them \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning\n",
    "%matplotlib inline\n",
    "\n",
    "import os #For looking through files\n",
    "import os.path #For managing file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Score Function\n",
    "\n",
    "The score function is a simple calculation developed by [Matt Taddy](https://arxiv.org/pdf/1504.07295.pdf) to calculate the likelihood that a given text would have been generated by a word-embedding model by summing the inner product between each pair of the text's word vectors. \n",
    "\n",
    "Here, we explore this using a model trained with millions of resumes from the CareerBuilder website (we can't share the private resumes...but we can share a model built with them :-):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_model  = gensim.models.word2vec.Word2Vec.load('../data/resumeAll.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the vacabularies of this model by building a word-index map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = resume_model.index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just load the sample and take a look at it. The sentences in each job description are already tokenized and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF = pandas.read_csv('../data/SampleJobAds.csv', index_col = False)\n",
    "#We need to convert the last couple columns from strings to lists\n",
    "sampleDF['tokenized_sents'] = sampleDF['tokenized_sents'].apply(lambda x: eval(x))\n",
    "sampleDF['normalized_sents'] = sampleDF['normalized_sents'].apply(lambda x: eval(x))\n",
    "sampleDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to calculate the likelihood of each job description. The idea is borrowed from [Matt Taddy](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/deepir.ipynb), who shows how a document can be characterized as the inner product of the distance between its words. In other words, this analysis will show which job ads are most likely to find an appropriate pool of workers in the resume bank that generated our word embedding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adprob(ad, model):\n",
    "    sen_scores = model.score(ad, len(ad))\n",
    "    ad_score = sen_scores.mean()\n",
    "    return ad_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this function to every job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampleDF['likelihood'] = sampleDF['normalized_sents'].apply(lambda x: adprob(x, resume_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the top 5 job descriptions that have the highest likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ad in sampleDF.sort_values(by = 'likelihood', ascending = False)['jobDescription'][:5]:\n",
    "    print (ad + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the bottom 5 job descriptions that have the lowest likelihood to be matched by the resumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ad in sampleDF.sort_values(by = 'likelihood')['jobDescription'][:5]:\n",
    "    print (ad + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for phrases corresponding to job skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adprob([[\"python\", \"programming\"]], resume_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adprob([[\"basic\", \"programming\"]], resume_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic programming appears to be more likely in this pool of resumes than python programming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do some simple statistics. Unfortunately, we don't have a large sample here. Nevertheless, let's first look at the mean likelihood score of each hiring organization. Some organizations will do well to hire on CareerBuilder...while others will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF.groupby(\"hiringOrganization_organizationName\")[['likelihood']].mean().sort_values('likelihood', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the mean likelihood of each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF.groupby(\"jobLocation_address_region\")[['likelihood']].mean().sort_values('likelihood', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would increase the sample size if you want to do a more serious study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1a*</span>\n",
    "\n",
    "<span style=\"color:red\">**Do only 1a or 1b.** Construct cells immediately below this that calculate the scores for a small sample of documents from outside your corpus to identify which are *closest* to your corpus. Then calculate the scores for a few phrases or sentences to identify the ones most likely to have appeared in your corpus. Interrogate patterns associated with these document/phrase scores (e.g., which companies produced job ads most or least likely to find jobseekers in the resume corpus?) What do these patterns suggest about the boundaries of your corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2011 - estimating the constraints to trade of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2013 - mineral resource trade in chile - contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2010 - international standards and trade - a r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2014 - services trade restrictiveness index (S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2011 - trade in tasks.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2017 - export controls and competitiveness in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2010 - the availability and cost of short-term...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2011 - dynamic gains from trade - the role of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2017 - digital trade - developing a framework ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2017 - mapping the participation of ASEAN smal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2013 - trade costs - what have we learned? - a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2010 - increasing the impact of trade expansio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2015 - contribution of trade facilitation meas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2014 - deep provisions in regional trade agree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2014 - transparency of export restrictions - a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2011 - trade and innovation - pharmaceuticals.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2013 - global value chains and developing coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2013 - export restrictions - benefits of trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2011 - comparative advantage and trade perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2015 - water in the GATS - methodology and res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2014 - export restrictions on raw materials - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2015 - the impact of services trade restrictiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2013 - state-owned enterprises - trade effects...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2013 - estimating the constraints to agricultu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2011 - agricultural trade and employment in so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2011 - trade and labour market outcomes in ger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2011 - global imbalances - trade effects and p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2010 - multilateralising regionalism - how pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name=\"../5-Reliability/abs...</td>\n",
       "      <td>2013 - trade facilitation indicators - the pot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2014 - services trade restrictiveness index (S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2013 - trade and labour market adjustment.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2012 - trade effects of exchange rates and the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2016 - services trade restrictiveness, mark-up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2011 - trade and occupational employment in me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2017 - OECD taxonomy of measures affecting tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2017 - trading firms and trading costs in serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2011 - trade in information and communications...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2011 - intra-firm trade - patterns, determinan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2015 - managing the minerals sector - implicat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2011 - trade and employment - the case of denm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name=\"../5-Reliability/abs...</td>\n",
       "      <td>2013 - different partners, different patterns ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2013 - multilateralising regionalism on govern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2011 - wage implications of trade liberalisati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2010 - trade and innovation - report on the ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2011 - a literature review on trade and inform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2017 - services in global value chains - trade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2017 - bringing together international trade a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2014 - small and medium-sized enterprises in g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2017 - services trade costs - tariff equivalen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2016 - trade-related international regulatory ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2012 - trade, employment and structural change...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2016 - GVCs, jobs and routine content of occup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2011 - trade and employment in japan.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2010 - policy complements to the strengthening...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2013 - global production networks and employme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2015 - services trade restrictiveness index (S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2011 - transparency mechanisms and non-tariff ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2011 - trade facilitation indicators - the imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2011 - exporting, employment, and skill upgrad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;_io.BufferedReader name='../5-Reliability/abs...</td>\n",
       "      <td>2015 - participation of developing countries i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name=\"../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "..                                                ...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name=\"../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "0   <_io.BufferedReader name='../5-Reliability/abs...   \n",
       "\n",
       "                                             category  \n",
       "0   2011 - estimating the constraints to trade of ...  \n",
       "0   2013 - mineral resource trade in chile - contr...  \n",
       "0   2010 - international standards and trade - a r...  \n",
       "0   2014 - services trade restrictiveness index (S...  \n",
       "0                           2011 - trade in tasks.txt  \n",
       "0   2017 - export controls and competitiveness in ...  \n",
       "0   2010 - the availability and cost of short-term...  \n",
       "0   2011 - dynamic gains from trade - the role of ...  \n",
       "0   2017 - digital trade - developing a framework ...  \n",
       "0   2017 - mapping the participation of ASEAN smal...  \n",
       "0   2013 - trade costs - what have we learned? - a...  \n",
       "0   2010 - increasing the impact of trade expansio...  \n",
       "0   2015 - contribution of trade facilitation meas...  \n",
       "0   2014 - deep provisions in regional trade agree...  \n",
       "0   2014 - transparency of export restrictions - a...  \n",
       "0   2011 - trade and innovation - pharmaceuticals.txt  \n",
       "0   2013 - global value chains and developing coun...  \n",
       "0   2013 - export restrictions - benefits of trans...  \n",
       "0   2011 - comparative advantage and trade perform...  \n",
       "0   2015 - water in the GATS - methodology and res...  \n",
       "0   2014 - export restrictions on raw materials - ...  \n",
       "0   2015 - the impact of services trade restrictiv...  \n",
       "0   2013 - state-owned enterprises - trade effects...  \n",
       "0   2013 - estimating the constraints to agricultu...  \n",
       "0   2011 - agricultural trade and employment in so...  \n",
       "0   2011 - trade and labour market outcomes in ger...  \n",
       "0   2011 - global imbalances - trade effects and p...  \n",
       "0   2010 - multilateralising regionalism - how pre...  \n",
       "0   2013 - trade facilitation indicators - the pot...  \n",
       "0   2014 - services trade restrictiveness index (S...  \n",
       "..                                                ...  \n",
       "0       2013 - trade and labour market adjustment.txt  \n",
       "0   2012 - trade effects of exchange rates and the...  \n",
       "0   2016 - services trade restrictiveness, mark-up...  \n",
       "0   2011 - trade and occupational employment in me...  \n",
       "0   2017 - OECD taxonomy of measures affecting tra...  \n",
       "0   2017 - trading firms and trading costs in serv...  \n",
       "0   2011 - trade in information and communications...  \n",
       "0   2011 - intra-firm trade - patterns, determinan...  \n",
       "0   2015 - managing the minerals sector - implicat...  \n",
       "0   2011 - trade and employment - the case of denm...  \n",
       "0   2013 - different partners, different patterns ...  \n",
       "0   2013 - multilateralising regionalism on govern...  \n",
       "0   2011 - wage implications of trade liberalisati...  \n",
       "0   2010 - trade and innovation - report on the ch...  \n",
       "0   2011 - a literature review on trade and inform...  \n",
       "0   2017 - services in global value chains - trade...  \n",
       "0   2017 - bringing together international trade a...  \n",
       "0   2014 - small and medium-sized enterprises in g...  \n",
       "0   2017 - services trade costs - tariff equivalen...  \n",
       "0   2016 - trade-related international regulatory ...  \n",
       "0   2012 - trade, employment and structural change...  \n",
       "0   2016 - GVCs, jobs and routine content of occup...  \n",
       "0            2011 - trade and employment in japan.txt  \n",
       "0   2010 - policy complements to the strengthening...  \n",
       "0   2013 - global production networks and employme...  \n",
       "0   2015 - services trade restrictiveness index (S...  \n",
       "0   2011 - transparency mechanisms and non-tariff ...  \n",
       "0   2011 - trade facilitation indicators - the imp...  \n",
       "0   2011 - exporting, employment, and skill upgrad...  \n",
       "0   2015 - participation of developing countries i...  \n",
       "\n",
       "[116 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absDir = '../5-Reliability/abstracts'\n",
    "\n",
    "absDF = pandas.DataFrame(columns = ['text','category'])\n",
    "\n",
    "\n",
    "for filename in os.listdir(absDir):\n",
    "    txt_file = open(absDir + '/' + filename, 'rb')\n",
    "    abDF = pandas.DataFrame(data = [''], columns = ['text'])\n",
    "    abDF['text'][0] = txt_file\n",
    "    abDF['category'] = filename\n",
    "    absDF = absDF.append(abDF, ignore_index = False)\n",
    "    \n",
    "absDF\n",
    "\n",
    "\n",
    "#    papersDF = papersDF.append(papDF, ignore_index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d9bc43132af4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpdf_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdfDir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpapDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mpapDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadPDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mpapDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpapersDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpapersDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpapDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d2adf477350e>\u001b[0m in \u001b[0;36mreadPDF\u001b[0;34m(pdfFile)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mpagenos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdfpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPDFPage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdfFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpagenos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxpages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxpages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcaching\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_extractable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mreturnedString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pdfminer/pdfinterp.py\u001b[0m in \u001b[0;36mprocess_page\u001b[0;34m(self, page)\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_contents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pdfminer/converter.py\u001b[0m in \u001b[0;36mend_page\u001b[0;34m(self, page)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLTPage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpageno\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pdfminer/layout.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self, laparams)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0mtextboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_textlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlaparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtextlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtextboxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_textboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlaparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtextboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0massigner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndexAssigner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pdfminer/layout.py\u001b[0m in \u001b[0;36mgroup_textboxes\u001b[0;34m(self, laparams, boxes)\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mother\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplane\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m                 \u001b[0mdists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0mplane\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplane\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pdfminer/utils.py\u001b[0m in \u001b[0;36mcsort\u001b[0;34m(objs, key)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;34m\"\"\"Order-preserving sorting function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pdfminer/utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;34m\"\"\"Order-preserving sorting function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pdfDir = '../4-Word-Embedding/papers'\n",
    "\n",
    "papersDF = pandas.DataFrame(columns = ['text','category'])\n",
    "\n",
    "\n",
    "for filename in os.listdir(pdfDir):\n",
    "    pdf_file = open(pdfDir + '/' + filename, 'rb')\n",
    "    papDF = pandas.DataFrame(data =[''], columns = ['text'])\n",
    "    papDF['text'][0] = readPDF(pdf_file)\n",
    "    papDF['category'] = filename\n",
    "    papersDF = papersDF.append(papDF, ignore_index = False)\n",
    "\n",
    "#for papersDir in (file for file in os.scandir(pdfDir) if not file.name.startswith('.') and file.is_file()):\n",
    "#    print(\"howdy\")\n",
    "#    papDF = pandas.DataFrame(data =[''], columns = ['text'])\n",
    "#    papDF['text'][0] = 'title'#readPDF(papersDir.path)\n",
    "#    papDF['category'] = papersDir.name\n",
    "#    papersDF = papersDF.append(papDF, ignore_index = False)\n",
    "\n",
    "papersDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code that aligns the dimensions of multiple embeddings arrayed over time or some other dimension and allow identification of semantic chanage as the word vectors change their loadings for focal words. This code comes from the approach piloted at Stanford by William Hamilton, Daniel Jurafsky and Jure Lescovec [here](https://arxiv.org/pdf/1605.09096.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_syn0norm(model):\n",
    "    \"\"\"since syn0norm is now depricated\"\"\"\n",
    "    return (model.wv.syn0 / np.sqrt((model.wv.syn0 ** 2).sum(-1))[..., np.newaxis]).astype(np.float32)\n",
    "\n",
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "    \"\"\"Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "    (With help from William. Thank you!)\n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "    base_embed = copy.copy(base_embed)\n",
    "    other_embed = copy.copy(other_embed)\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "    # get the embedding matrices\n",
    "    base_vecs = calc_syn0norm(in_base_embed)\n",
    "    other_vecs = calc_syn0norm(in_other_embed)\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs) \n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v) \n",
    "    # Replace original array with modified one\n",
    "    # i.e. multiplying the embedding matrix (syn0norm)by \"ortho\"\n",
    "    other_embed.wv.syn0norm = other_embed.wv.syn0 = (calc_syn0norm(other_embed)).dot(ortho)\n",
    "    return other_embed\n",
    "    \n",
    "def intersection_align_gensim(m1,m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.vocab.keys())\n",
    "    vocab_m2 = set(m2.wv.vocab.keys())\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1&vocab_m2\n",
    "    if words: common_vocab&=set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1-common_vocab and not vocab_m2-common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.vocab[w].count + m2.wv.vocab[w].count,reverse=True)\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1,m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.vocab[w].index for w in common_vocab]\n",
    "        old_arr = calc_syn0norm(m)\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.syn0norm = m.wv.syn0 = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        m.index2word = common_vocab\n",
    "        old_vocab = m.wv.vocab\n",
    "        new_vocab = {}\n",
    "        for new_index,word in enumerate(common_vocab):\n",
    "            old_vocab_obj=old_vocab[word]\n",
    "            new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
    "        m.wv.vocab = new_vocab\n",
    "\n",
    "    return (m1,m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to explore this, let's get some data that follows a time trend. We'll look at conference proceedings from the American Society for Clinical Oncologists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascoDF = pandas.read_csv(\"../data/ASCO_abstracts.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for wor2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ascoDF['tokenized_sents'] = ascoDF['Body'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "ascoDF['normalized_sents'] = ascoDF['tokenized_sents'].apply(lambda x: [lucem_illud.normalizeTokens(s, stopwordLst = lucem_illud.stop_words_basic) for s in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be creating many embeddings so we have created this function to do most of the work. It creates two collections of embeddings, one the original and one the aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareModels(df, category, sort = True):\n",
    "    \"\"\"If you are using time as your category sorting is important\"\"\"\n",
    "    embeddings_raw = {}\n",
    "    cats = sorted(set(df[category]))\n",
    "    for cat in cats:\n",
    "        #This can take a while\n",
    "        print(\"Embedding {}\".format(cat), end = '\\r')\n",
    "        subsetDF = df[df[category] == cat]\n",
    "        #You might want to change the W2V parameters\n",
    "        embeddings_raw[cat] = gensim.models.word2vec.Word2Vec(subsetDF['normalized_sents'].sum())\n",
    "    #These are much quicker\n",
    "    embeddings_aligned = {}\n",
    "    for catOuter in cats:\n",
    "        embeddings_aligned[catOuter] = [embeddings_raw[catOuter]]\n",
    "        for catInner in cats:\n",
    "            embeddings_aligned[catOuter].append(smart_procrustes_align_gensim(embeddings_aligned[catOuter][-1], embeddings_raw[catInner]))\n",
    "    return embeddings_raw, embeddings_aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawEmbeddings, comparedEmbeddings = compareModels(ascoDF, 'Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compare them across all permutions so we will define another function to help, we will be using 1 - cosine similarity as that gives a more intitive range of 0-2 with low values meaning little change and high meaning lots of change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDivergenceDF(word, embeddingsDict):\n",
    "    dists = []\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    dists = {}\n",
    "    for cat in cats:\n",
    "        dists[cat] = []\n",
    "        for embed in embeddingsDict[cat][1:]:\n",
    "            dists[cat].append(np.abs(1 - sklearn.metrics.pairwise.cosine_similarity(np.expand_dims(embeddingsDict[cat][0][word], axis = 0),\n",
    "                                                                             np.expand_dims(embed[word], axis = 0))[0,0]))\n",
    "    return pandas.DataFrame(dists, index = cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at a couple words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetWord = 'breast'\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetWord = 'triple'\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask which words changed the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDiverence(word, embeddingsDict):\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    \n",
    "    dists = []\n",
    "    for embed in embeddingsDict[cats[0]][1:]:\n",
    "        dists.append(1 - sklearn.metrics.pairwise.cosine_similarity(np.expand_dims(embeddingsDict[cats[0]][0][word], axis = 0), np.expand_dims(embed[word], axis = 0))[0,0])\n",
    "    return sum(dists)\n",
    "\n",
    "def findMostDivergent(embeddingsDict):\n",
    "    words = []\n",
    "    for embeds in embeddingsDict.values():\n",
    "        for embed in embeds:\n",
    "            words += list(embed.wv.vocab.keys())\n",
    "    words = set(words)\n",
    "    print(\"Found {} words to compare\".format(len(words)))\n",
    "    return sorted([(w, findDiverence(w, embeddingsDict)) for w in words], key = lambda x: x[1], reverse=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDivergences = findMostDivergent(comparedEmbeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most divergent words are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDivergences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDivergences[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetWord = wordDivergences[0][0]\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetWord = wordDivergences[-1][0]\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1b*</span>\n",
    "\n",
    "<span style=\"color:red\">**Do only 3a or 3b.** Construct cells immediately below this that align word embeddings over time. Interrogate the spaces that result and ask which words change most of the whole period. What does this reveal about the social game underlying your space?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
