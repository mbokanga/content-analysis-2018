{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Word Embeddings\n",
    "\n",
    "This week, we build on last week's topic modeling techniques by taking a text corpus we have developed, specifying an underlying number of dimensions, and training a model with a neural network auto-encoder (one of Google's word2vec  algorithms) that best describes corpus words in their local linguistic contexts, and exploring their locations in the resulting space to learn about the discursive culture that produced them. Documents here are represented as densely indexed locations in dimensions, rather than sparse mixtures of topics (as in LDA topic modeling), so that distances between those documents (and words) are consistently superior, though they require the full vector of dimension loadings (rather than just a few selected topic loadings) to describe. We will explore these spaces to understand complex, semantic relationships between words, index documents with descriptive words, identify the likelihood that a given document would have been produced by a given vector model, and explore how semantic categories can help us understand the cultures that produced them.\n",
    "\n",
    "For this notebook we will be using the following packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "import gensim#For word2vec, etc\n",
    "import requests #For downloading our datasets\n",
    "import nltk #For stop words and stemmers\n",
    "import numpy as np #For arrays\n",
    "import pandas #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "import sklearn.metrics.pairwise #For cosine similarity\n",
    "import sklearn.manifold #For T-SNE\n",
    "import sklearn.decomposition #For PCA\n",
    "\n",
    "#gensim uses a couple of deprecated features\n",
    "#we can't do anything about them so lets ignore them \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning\n",
    "%matplotlib inline\n",
    "\n",
    "import os #For looking through files\n",
    "import os.path #For managing file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting our corpora\n",
    "\n",
    "Instead of downloading our corpora, we have download them in advance; a subset of the [senate press releases](https://github.com/lintool/GrimmerSenatePressReleases) are in `grimmerPressReleases`. We will load them into a DataFrame, using a function from a couple weeks ago `loadTextDirectory`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25Jan2007Kennedy4.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE    FACT SHEET I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20Oct2005Kennedy14.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE     As this bil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03Mar2007Kennedy7.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE     Ann Coulter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10Jul2007Kennedy8.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE  Thank you all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20Jul2007Kennedy14.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE  WASHINGTON  DC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     text\n",
       "25Jan2007Kennedy4.txt            FOR IMMEDIATE RELEASE    FACT SHEET I...\n",
       "20Oct2005Kennedy14.txt           FOR IMMEDIATE RELEASE     As this bil...\n",
       "03Mar2007Kennedy7.txt            FOR IMMEDIATE RELEASE     Ann Coulter...\n",
       "10Jul2007Kennedy8.txt            FOR IMMEDIATE RELEASE  Thank you all ...\n",
       "20Jul2007Kennedy14.txt           FOR IMMEDIATE RELEASE  WASHINGTON  DC..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kennedyDF = lucem_illud.loadTextDirectory('../data/grimmerPressReleases/Kennedy/')\n",
    "kennedyDF[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets us one Senator's data, but with no metadata, we can add a category column with a simple command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25Jan2007Kennedy4.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE    FACT SHEET I...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20Oct2005Kennedy14.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE     As this bil...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03Mar2007Kennedy7.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE     Ann Coulter...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10Jul2007Kennedy8.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE  Thank you all ...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20Jul2007Kennedy14.txt</th>\n",
       "      <td>FOR IMMEDIATE RELEASE  WASHINGTON  DC...</td>\n",
       "      <td>Kennedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     text  \\\n",
       "25Jan2007Kennedy4.txt            FOR IMMEDIATE RELEASE    FACT SHEET I...   \n",
       "20Oct2005Kennedy14.txt           FOR IMMEDIATE RELEASE     As this bil...   \n",
       "03Mar2007Kennedy7.txt            FOR IMMEDIATE RELEASE     Ann Coulter...   \n",
       "10Jul2007Kennedy8.txt            FOR IMMEDIATE RELEASE  Thank you all ...   \n",
       "20Jul2007Kennedy14.txt           FOR IMMEDIATE RELEASE  WASHINGTON  DC...   \n",
       "\n",
       "                       category  \n",
       "25Jan2007Kennedy4.txt   Kennedy  \n",
       "20Oct2005Kennedy14.txt  Kennedy  \n",
       "03Mar2007Kennedy7.txt   Kennedy  \n",
       "10Jul2007Kennedy8.txt   Kennedy  \n",
       "20Jul2007Kennedy14.txt  Kennedy  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kennedyDF['category'] = 'Kennedy'\n",
    "kennedyDF[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also will be wanting to load all the senators so we will need to loop over all the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19Mar2007Kyl117.txt</th>\n",
       "      <td>SECURITY ON STRIKE\\n  The U.S. Senate recently...</td>\n",
       "      <td>Kyl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10Nov2005Kyl304.txt</th>\n",
       "      <td>SEN. KYL AMENDMENT ON HUMVEE UPGRADE PASSES SE...</td>\n",
       "      <td>Kyl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6Dec2007Kyl46.txt</th>\n",
       "      <td>SEN. KYL ELECTED ASSISTANT SENATE REPUBLICAN L...</td>\n",
       "      <td>Kyl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29Aug2005Kyl347.txt</th>\n",
       "      <td>SEN. KYL: GOOD NEWS AND BAD NEWS FOR BACK TO S...</td>\n",
       "      <td>Kyl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11Sep2006Kyl161.txt</th>\n",
       "      <td>SENATE COMMEMORATES FIVE YEAR MARK OF 9/11 ATT...</td>\n",
       "      <td>Kyl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "19Mar2007Kyl117.txt  SECURITY ON STRIKE\\n  The U.S. Senate recently...   \n",
       "10Nov2005Kyl304.txt  SEN. KYL AMENDMENT ON HUMVEE UPGRADE PASSES SE...   \n",
       "6Dec2007Kyl46.txt    SEN. KYL ELECTED ASSISTANT SENATE REPUBLICAN L...   \n",
       "29Aug2005Kyl347.txt  SEN. KYL: GOOD NEWS AND BAD NEWS FOR BACK TO S...   \n",
       "11Sep2006Kyl161.txt  SENATE COMMEMORATES FIVE YEAR MARK OF 9/11 ATT...   \n",
       "\n",
       "                    category  \n",
       "19Mar2007Kyl117.txt      Kyl  \n",
       "10Nov2005Kyl304.txt      Kyl  \n",
       "6Dec2007Kyl46.txt        Kyl  \n",
       "29Aug2005Kyl347.txt      Kyl  \n",
       "11Sep2006Kyl161.txt      Kyl  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDir = '../data/grimmerPressReleases'\n",
    "\n",
    "senReleasesDF = pandas.DataFrame()\n",
    "\n",
    "for senatorDir in (file for file in os.scandir(dataDir) if not file.name.startswith('.') and file.is_dir()):\n",
    "    senDF = lucem_illud.loadTextDirectory(senatorDir.path)\n",
    "    senDF['category'] = senatorDir.name\n",
    "    senReleasesDF = senReleasesDF.append(senDF, ignore_index = False)\n",
    "\n",
    "senReleasesDF[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to remove stop words and stem. Tokenizing requires two steps. Word2Vec needs to retain the sentence structure so as to capture a \"continuous bag of words (CBOW)\" and all of the skip-grams within a word window. The algorithm tries to preserve the distances induced by one of these two local structures. This is very different from clustering and LDA topic modeling which extract unordered words alone. As such, tokenizing is slightly more involved, but we can still use `lucem_illud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19Mar2007Kyl117.txt</th>\n",
       "      <td>SECURITY ON STRIKE\\n  The U.S. Senate recently...</td>\n",
       "      <td>Kyl</td>\n",
       "      <td>[[SECURITY, ON, STRIKE, The, U.S., Senate, rec...</td>\n",
       "      <td>[[security, strike, senate, recently, approved...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10Nov2005Kyl304.txt</th>\n",
       "      <td>SEN. KYL AMENDMENT ON HUMVEE UPGRADE PASSES SE...</td>\n",
       "      <td>Kyl</td>\n",
       "      <td>[[SEN., KYL, AMENDMENT, ON, HUMVEE, UPGRADE, P...</td>\n",
       "      <td>[[kyl, amendment, humvee, upgrade, passes, sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6Dec2007Kyl46.txt</th>\n",
       "      <td>SEN. KYL ELECTED ASSISTANT SENATE REPUBLICAN L...</td>\n",
       "      <td>Kyl</td>\n",
       "      <td>[[SEN., KYL, ELECTED, ASSISTANT, SENATE, REPUB...</td>\n",
       "      <td>[[kyl, elected, assistant, senate, republican,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29Aug2005Kyl347.txt</th>\n",
       "      <td>SEN. KYL: GOOD NEWS AND BAD NEWS FOR BACK TO S...</td>\n",
       "      <td>Kyl</td>\n",
       "      <td>[[SEN., KYL, :, GOOD, NEWS, AND, BAD, NEWS, FO...</td>\n",
       "      <td>[[kyl, good, news, bad, news, back, school, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11Sep2006Kyl161.txt</th>\n",
       "      <td>SENATE COMMEMORATES FIVE YEAR MARK OF 9/11 ATT...</td>\n",
       "      <td>Kyl</td>\n",
       "      <td>[[SENATE, COMMEMORATES, FIVE, YEAR, MARK, OF, ...</td>\n",
       "      <td>[[senate, commemorates, five, year, mark, atta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "19Mar2007Kyl117.txt  SECURITY ON STRIKE\\n  The U.S. Senate recently...   \n",
       "10Nov2005Kyl304.txt  SEN. KYL AMENDMENT ON HUMVEE UPGRADE PASSES SE...   \n",
       "6Dec2007Kyl46.txt    SEN. KYL ELECTED ASSISTANT SENATE REPUBLICAN L...   \n",
       "29Aug2005Kyl347.txt  SEN. KYL: GOOD NEWS AND BAD NEWS FOR BACK TO S...   \n",
       "11Sep2006Kyl161.txt  SENATE COMMEMORATES FIVE YEAR MARK OF 9/11 ATT...   \n",
       "\n",
       "                    category  \\\n",
       "19Mar2007Kyl117.txt      Kyl   \n",
       "10Nov2005Kyl304.txt      Kyl   \n",
       "6Dec2007Kyl46.txt        Kyl   \n",
       "29Aug2005Kyl347.txt      Kyl   \n",
       "11Sep2006Kyl161.txt      Kyl   \n",
       "\n",
       "                                                       tokenized_sents  \\\n",
       "19Mar2007Kyl117.txt  [[SECURITY, ON, STRIKE, The, U.S., Senate, rec...   \n",
       "10Nov2005Kyl304.txt  [[SEN., KYL, AMENDMENT, ON, HUMVEE, UPGRADE, P...   \n",
       "6Dec2007Kyl46.txt    [[SEN., KYL, ELECTED, ASSISTANT, SENATE, REPUB...   \n",
       "29Aug2005Kyl347.txt  [[SEN., KYL, :, GOOD, NEWS, AND, BAD, NEWS, FO...   \n",
       "11Sep2006Kyl161.txt  [[SENATE, COMMEMORATES, FIVE, YEAR, MARK, OF, ...   \n",
       "\n",
       "                                                      normalized_sents  \n",
       "19Mar2007Kyl117.txt  [[security, strike, senate, recently, approved...  \n",
       "10Nov2005Kyl304.txt  [[kyl, amendment, humvee, upgrade, passes, sen...  \n",
       "6Dec2007Kyl46.txt    [[kyl, elected, assistant, senate, republican,...  \n",
       "29Aug2005Kyl347.txt  [[kyl, good, news, bad, news, back, school, st...  \n",
       "11Sep2006Kyl161.txt  [[senate, commemorates, five, year, mark, atta...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply our functions, notice each row is a list of lists now\n",
    "senReleasesDF['tokenized_sents'] = senReleasesDF['text'].apply(lambda x: \n",
    "                                                               [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "senReleasesDF['normalized_sents'] = senReleasesDF['tokenized_sents'].apply(lambda x: \n",
    "                                                                           [lucem_illud.normalizeTokens(s, \n",
    "                                                                                           stopwordLst = lucem_illud.stop_words_basic, \n",
    "                                                                                           stemmer = None) \n",
    "                                                                            for s in x])\n",
    "\n",
    "senReleasesDF[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "We will be using the gensim implementation of [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec).\n",
    "\n",
    "To load our data, we give all the sentences to the trainer. We just need to add the words as a list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "senReleasesW2V = gensim.models.word2vec.Word2Vec(senReleasesDF['normalized_sents'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the word2vec object, the words each have a vector. To access the vector directly, use the square braces (`__getitem__`) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 100 dimesional vector:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.23311073e-01,  2.91070901e-02,  3.72806787e-01,  1.15513980e+00,\n",
       "       -8.52942560e-03, -1.35338986e+00, -2.21135545e+00, -1.93669870e-01,\n",
       "        2.62513542e+00, -1.59287155e+00, -2.89358616e+00, -1.47005051e-01,\n",
       "       -3.10299182e+00, -1.29270256e+00,  1.03244078e+00, -2.17692924e+00,\n",
       "        8.69692206e-01,  4.21423554e-01,  8.22465777e-01, -1.47246122e+00,\n",
       "        1.40730023e+00,  1.84333014e+00,  4.17263478e-01, -1.27221406e+00,\n",
       "        3.13766456e+00,  1.34641516e+00,  8.84112358e-01,  3.92642498e-01,\n",
       "       -9.91539896e-01, -4.05030608e-01, -2.13964677e+00,  8.43997061e-01,\n",
       "       -9.96574402e-01,  1.99502635e+00,  4.84043658e-01, -1.35057640e+00,\n",
       "       -3.27576637e-01, -1.29662156e+00, -6.85254097e-01, -5.37725985e-01,\n",
       "       -1.12437773e+00, -6.81538880e-01,  2.01327729e+00, -5.59828222e-01,\n",
       "       -2.57545114e-01, -7.57361889e-01, -9.20466125e-01,  5.92852890e-01,\n",
       "        1.81559014e+00,  1.39916015e+00,  1.64878398e-01,  4.66768503e-01,\n",
       "       -1.12871993e+00, -1.04051542e+00,  1.31430256e+00,  2.95403600e+00,\n",
       "       -3.59144002e-01, -2.72797370e+00, -1.58925998e+00,  4.76813436e-01,\n",
       "       -7.35257268e-01, -1.37311983e+00, -3.88507848e-03, -1.18441425e-01,\n",
       "       -7.87186563e-01, -3.52442497e-04, -1.33478761e+00,  1.40155435e+00,\n",
       "       -6.35669947e-01, -9.31386054e-01, -2.12224245e-01,  3.30897048e-02,\n",
       "        1.10452986e+00,  1.89574540e+00, -4.74135429e-02,  1.68455601e+00,\n",
       "        7.47375190e-01,  1.67155303e-02,  3.08406878e+00,  1.64906895e+00,\n",
       "       -1.28983259e+00,  2.63092852e+00,  1.68381393e+00, -2.08527112e+00,\n",
       "       -1.04182744e+00, -3.33507061e-01, -1.67431211e+00, -1.24667370e+00,\n",
       "       -2.61635876e+00,  2.06247553e-01,  2.52619028e+00, -3.80118370e-01,\n",
       "       -4.66753125e-01, -2.68777221e-01,  6.54484853e-02,  2.59489965e+00,\n",
       "       -4.47536916e-01,  1.90678275e+00,  9.27909791e-01, -1.47107124e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"A {} dimesional vector:\".format(senReleasesW2V['president'].shape[0]))\n",
    "senReleasesW2V['president']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want the full matrix, `syn0` stores all the vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0347886e+00,  1.4148208e+00,  7.0726222e-01, ...,\n",
       "        -1.0268974e+00, -1.6531397e+00, -2.3061719e+00],\n",
       "       [-1.6438064e-01, -3.0502640e-03, -1.6143703e-01, ...,\n",
       "         2.5118504e+00, -5.4739881e-01, -2.1059639e+00],\n",
       "       [-1.9680681e-03, -4.3824208e-01, -7.9360265e-01, ...,\n",
       "         3.1762844e-01, -1.4682245e+00,  2.8229699e-02],\n",
       "       ...,\n",
       "       [ 5.3340020e-03,  3.0627726e-02,  8.0126472e-02, ...,\n",
       "         2.0445121e-02, -8.2088977e-02, -4.1528260e-03],\n",
       "       [ 1.5816940e-02,  3.3116307e-02,  6.6006541e-02, ...,\n",
       "         4.6429511e-02, -7.7461740e-03, -5.8256779e-02],\n",
       "       [ 1.4633126e-02,  4.6324857e-02,  1.2588888e-01, ...,\n",
       "         7.5531021e-02, -3.4731273e-02, -7.7196971e-02]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.wv.syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, `index2word` lets you translate from the matrix to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'american'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.wv.index2word[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at a few things that come from the word vectors. The first is to find similar vectors (cosine similarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('administration', 0.8580595850944519),\n",
       " ('presidents', 0.7659381628036499),\n",
       " ('administrations', 0.7001156806945801),\n",
       " ('george', 0.6200462579727173),\n",
       " ('cheney', 0.6080427765846252),\n",
       " ('republican', 0.5937576293945312),\n",
       " ('pen', 0.5875585079193115),\n",
       " ('lamont', 0.5784647464752197),\n",
       " ('rollback', 0.5615742206573486),\n",
       " ('walker', 0.5494815707206726)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.most_similar('president')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('afghanistan', 0.6965972781181335),\n",
       " ('sending', 0.6858476996421814),\n",
       " ('terror', 0.6854323744773865),\n",
       " ('troop', 0.6731855273246765),\n",
       " ('chaos', 0.6651113033294678),\n",
       " ('misguided', 0.6605321168899536),\n",
       " ('escalation', 0.6538366079330444),\n",
       " ('foment', 0.6500982642173767),\n",
       " ('battle', 0.6476494073867798),\n",
       " ('descending', 0.6428756713867188)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.most_similar('war')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can get this directly (calculated slightly differently):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_difference(embedding,word1,word2):\n",
    "    return sklearn.metrics.pairwise.cosine_similarity(embedding[word1].reshape(1,-1),embedding[word2].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5875652]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_difference(senReleasesW2V, 'war', 'unwinnable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find which word least matches the others within a word set (cosine similarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'washington'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.doesnt_match(['administration', 'administrations', 'presidents', 'president', 'washington'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find which word best matches the result of a semantic *equation* (here, we seek the words whose vectors best fit the missing entry from the equation: **X + Y - Z = _**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('veto', 0.7101362347602844),\n",
       " ('bushs', 0.6949179172515869),\n",
       " ('bush', 0.6945303678512573),\n",
       " ('took', 0.6733492612838745),\n",
       " ('playbook', 0.6699895262718201),\n",
       " ('vetoed', 0.6633447408676147),\n",
       " ('presidents', 0.6619688272476196),\n",
       " ('signed', 0.6599994897842407),\n",
       " ('signing', 0.6599656343460083),\n",
       " ('assad', 0.6389331221580505)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.most_similar(positive=['clinton', 'republican'], negative = ['democrat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that **Clinton + Republican - Democrat = Bush**. In other words, in this dataset and period, **Clinton** was to **Democrat** as **Bush** was to **Republican**. Whoah!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " But how do we argue that these are stable distances or associations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing Credible or Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boostrapping approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose well-established bootstrapping and subsampling methods to nonparametrically demonstrate the stability and significance of word associations within our embedding model. These allow us to establish conservative confidence intervals to both (a) distances between words in a model and (b) projections of words onto an induced dimension (e.g., man-woman). If we assume that the texts (e.g., newspapers, books) underlying our word embedding model are observations drawn from an independent and identically distributed (i.i.d.) population of cultural observations, then bootstrapping allows us to estimate the variance of word distances and projections by measuring those properties through sampling the empirical distribution of texts with replacement (Efron and Tibshirani 1994; Efron 2003). Operationally, if we wanted to bootstrap a 90% confidence interval of a word-word distance or word-dimension projection, we would sample a corpus the same size as the original corpus, but with replacement, 20 times, estimate word embedding models on each sample. Then we take the 2nd order (2nd smallest) statistic $s_{(2)}$--either distance or projection--as our confidence interval’s lower bound, and 19th order statistic $s_{(19)}$ as its upper bound. The distance between $s_{(2)}$ and $s_{(19)}$ across 20 bootstrap samples span the 5th to the 95th percentiles of the statistic’s variance, bounding the 90th confidence interval. A 95% confidence interval would span $s_{(2)}$ and $s_{(39)}$ in word embedding distances or projections estimated on 40 bootstrap samples of a corpus, tracing the 2.5th to 97.5th percentiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.31148726,\n",
       " 0.3326747,\n",
       " 0.35395575,\n",
       " 0.37102804,\n",
       " 0.378289,\n",
       " 0.41180086,\n",
       " 0.41599232,\n",
       " 0.42050356,\n",
       " 0.42421365,\n",
       " 0.4350288,\n",
       " 0.46107912,\n",
       " 0.4681642,\n",
       " 0.51285875,\n",
       " 0.5153641,\n",
       " 0.51580936,\n",
       " 0.5274905,\n",
       " 0.54890007,\n",
       " 0.55744606,\n",
       " 0.6679215]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimatesB=[]\n",
    "for x in range(20):\n",
    "    senReleasesW2VB = gensim.models.word2vec.Word2Vec(senReleasesDF['normalized_sents'].sample(frac=1.0, replace=True).sum())\n",
    "    try:\n",
    "        estimatesB.append(cos_difference(senReleasesW2VB, 'war', 'unwinnable')[0,0])\n",
    "    except KeyError:\n",
    "        #Missing one of the words from the vocab\n",
    "        pass\n",
    "                                                      \n",
    "estimatesB.sort()         \n",
    "estimatesB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 90% confidence interval for the cosine distance between war and unwinnable is:\n",
      " 0.3326747 0.55744606\n"
     ]
    }
   ],
   "source": [
    "print(\"The 90% confidence interval for the cosine distance between war and unwinnable is:\\n\",estimatesB[1], estimatesB[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the corpus is large, then we may take a subsampling approach, which randomly partitions the corpus into non-overlapping samples, then estimates the word-embedding models on these subsets and calculates confidence intervals as a function of the empirical distribution of distance or projection statistics and number of texts in the subsample (Politis and Romano 1997). Subsampling requires the same i.i.d. assumption as the bootrap (Politis and Romano 1992; Politis and Romano 1994). For 90% confidence intervals, we randomly partition the corpus into 20 subcorpora, then calculate $B^k=\\sqrt{\\tau_k}\\left(s^k-\\bar{s}\\right)$ for each $k$th sample, where $k$ is the number of texts and $s^k$ is the embedding distance or projection for the $k$th sample, and $\\bar{s}$ is the average statistic for all samples. The 90% confidence interval spans the 5th to 95th percentile variances, inscribed by $\\bar{s}-\\frac{B_{(19)}^k}{\\sqrt{\\tau}}$ and $\\bar{s}-\\frac{B_{(2)}^k}{\\sqrt{\\tau}}$, where $\\tau$ the number of texts in the total corpus and $s$ is the average statistic across all subsamples. As with bootrapping, a 95% confidence interval would require 40 subsamples; a 99% confidence would require 200 (.5th to 99.5th percentiles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-03d107c17b3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msenReleasesDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_indices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_indices' is not defined"
     ]
    }
   ],
   "source": [
    "senReleasesDF[sample_indices == i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "sample_indices = np.random.randint(0,n_samples,(len(senReleasesDF),))\n",
    "\n",
    "s_k =np.array([])\n",
    "tau_k=np.array([])\n",
    "\n",
    "for i in range(n_samples):\n",
    "    sample_w2v = gensim.models.word2vec.Word2Vec(senReleasesDF[sample_indices == i]['normalized_sents'].sum())\n",
    "    try:\n",
    "        #Need to use words present in most samples\n",
    "        s_k = np.append(s_k, cos_difference(sample_w2v, 'war', 'responsibility')[0,0])\n",
    "    except KeyError:\n",
    "        pass\n",
    "    else:\n",
    "        tau_k = np.append(tau_k, len(senReleasesDF[sample_indices == i]))\n",
    "\n",
    "print(s_k)\n",
    "print(tau_k)\n",
    "\n",
    "tau = tau_k.sum()\n",
    "s = s_k.mean()\n",
    "B_k = np.sqrt(tau_k) * s_k-s_k.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The 90% confidence interval for the cosine distance between war and responsibility is:\\n\",s-B_k[-2]/np.sqrt(tau), s-B_k[1]/np.sqrt(tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save the vectors for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senReleasesW2V.save(\"senpressreleasesWORD2Vec.mm\")\n",
    "#Load with senReleasesW2V = gensim.models.word2vec.Word2Vec.load('senpressreleasesWORD2Vec.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use dimension reduction to visulize the vectors. We will start by selecting a subset we want to plot. Let's look at the top words from the set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numWords = 150\n",
    "targetWords = senReleasesW2V.wv.index2word[:numWords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then extract their vectors and create our own smaller matrix that preserved the distances from the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsSubMatrix = []\n",
    "for word in targetWords:\n",
    "    wordsSubMatrix.append(senReleasesW2V[word])\n",
    "wordsSubMatrix = np.array(wordsSubMatrix)\n",
    "wordsSubMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use PCA to reduce the dimesions (e.g., to 50), and [T-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to project them down to the two we will visualize. We note that this is nondeterministic process, and so you can repeat and achieve alternative projectsions/visualizations of the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaWords = sklearn.decomposition.PCA(n_components = 50).fit(wordsSubMatrix)\n",
    "reducedPCA_data = pcaWords.transform(wordsSubMatrix)\n",
    "#T-SNE is theoretically better, but you should experiment\n",
    "tsneWords = sklearn.manifold.TSNE(n_components = 2, early_exaggeration = 25).fit_transform(reducedPCA_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can plot the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "plt.scatter(tsneWords[:, 0], tsneWords[:, 1], alpha = 0)#Making the points invisible \n",
    "for i, word in enumerate(targetWords):\n",
    "    ax.annotate(word, \n",
    "                (tsneWords[:, 0][i],tsneWords[:, 1][i]), \n",
    "                size =  20 * (numWords - i) / numWords, \n",
    "                alpha = .8 * (numWords - i) / numWords + .2)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My visualization above puts ``iraq`` next to ``time`` and ``bill`` near ``help``. <img src='../data/examplewordcloud.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if a new senator come along with whose names starts with K? Could we analyse their releases too without rerunning the entire embedding? Lets try with Cardin in `../data/grimmerPressReleases_extra/Cardin`. First we need to load and proccess the releases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21Jun2007Cardin189.txt</th>\n",
       "      <td>...</td>\n",
       "      <td>Cardin</td>\n",
       "      <td>[[CARDIN, DEEPLY, DISAPPOINTED, BY, PRESIDENT,...</td>\n",
       "      <td>[[cardin, deeply, disappointed, president, dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07Nov2007Cardin47.txt</th>\n",
       "      <td>...</td>\n",
       "      <td>Cardin</td>\n",
       "      <td>[[SENATE, COMMITTEE, OVERWHELMINGLY, APPROVES,...</td>\n",
       "      <td>[[senate, committee, overwhelmingly, approves,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06Sep2007Cardin126.txt</th>\n",
       "      <td>...</td>\n",
       "      <td>Cardin</td>\n",
       "      <td>[[CARDIN, HAILS, COMMITTEE, PASSAGE, OF, BILL,...</td>\n",
       "      <td>[[cardin, hails, committee, passage, bill, end...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24Jul2007Cardin165.txt</th>\n",
       "      <td>...</td>\n",
       "      <td>Cardin</td>\n",
       "      <td>[[SENATOR, CARDIN, CALLS, ON, ATTORNEY, GENERA...</td>\n",
       "      <td>[[senator, cardin, calls, attorney, general, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24Apr2007Cardin234.txt</th>\n",
       "      <td>...</td>\n",
       "      <td>Cardin</td>\n",
       "      <td>[[CARDIN, AND, MIKULKSI, INTRODUCE, BILL, TO, ...</td>\n",
       "      <td>[[cardin, mikulksi, introduce, bill, give, sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20Jun2007Cardin193.txt</th>\n",
       "      <td>...</td>\n",
       "      <td>Cardin</td>\n",
       "      <td>[[CARDIN, EXPRESSES, DISAPPOINTMENT, ON, FAILU...</td>\n",
       "      <td>[[cardin, expresses, disappointment, failure, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10Sep2007Cardin116.txt</th>\n",
       "      <td>...</td>\n",
       "      <td>Cardin</td>\n",
       "      <td>[[CARDIN, DEMANDS, CORRECTION, TO, HELP, SENIO...</td>\n",
       "      <td>[[cardin, demands, correction, help, seniors, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12Jul2007Cardin174.txt</th>\n",
       "      <td>...</td>\n",
       "      <td>Cardin</td>\n",
       "      <td>[[CARDIN, MIKULSKI, ANNOUNCE, 144, 200, FOR, C...</td>\n",
       "      <td>[[cardin, mikulski, announce, cumberland, vall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02Mar2007Cardin276.txt</th>\n",
       "      <td>...</td>\n",
       "      <td>Cardin</td>\n",
       "      <td>[[CARDIN, MIKULSKI, ANNOUNCE, 365, 744, IN, FE...</td>\n",
       "      <td>[[cardin, mikulski, announce, federal, funding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06Dec2007Cardin21.txt</th>\n",
       "      <td>...</td>\n",
       "      <td>Cardin</td>\n",
       "      <td>[[CARDIN, MIKULSKI, ANNOUNCE, FEDERAL, FUNDING...</td>\n",
       "      <td>[[cardin, mikulski, announce, federal, funding...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     text  \\\n",
       "21Jun2007Cardin189.txt                                                ...   \n",
       "07Nov2007Cardin47.txt                                                 ...   \n",
       "06Sep2007Cardin126.txt                                                ...   \n",
       "24Jul2007Cardin165.txt                                                ...   \n",
       "24Apr2007Cardin234.txt                                                ...   \n",
       "20Jun2007Cardin193.txt                                                ...   \n",
       "10Sep2007Cardin116.txt                                                ...   \n",
       "12Jul2007Cardin174.txt                                                ...   \n",
       "02Mar2007Cardin276.txt                                                ...   \n",
       "06Dec2007Cardin21.txt                                                 ...   \n",
       "\n",
       "                       category  \\\n",
       "21Jun2007Cardin189.txt   Cardin   \n",
       "07Nov2007Cardin47.txt    Cardin   \n",
       "06Sep2007Cardin126.txt   Cardin   \n",
       "24Jul2007Cardin165.txt   Cardin   \n",
       "24Apr2007Cardin234.txt   Cardin   \n",
       "20Jun2007Cardin193.txt   Cardin   \n",
       "10Sep2007Cardin116.txt   Cardin   \n",
       "12Jul2007Cardin174.txt   Cardin   \n",
       "02Mar2007Cardin276.txt   Cardin   \n",
       "06Dec2007Cardin21.txt    Cardin   \n",
       "\n",
       "                                                          tokenized_sents  \\\n",
       "21Jun2007Cardin189.txt  [[CARDIN, DEEPLY, DISAPPOINTED, BY, PRESIDENT,...   \n",
       "07Nov2007Cardin47.txt   [[SENATE, COMMITTEE, OVERWHELMINGLY, APPROVES,...   \n",
       "06Sep2007Cardin126.txt  [[CARDIN, HAILS, COMMITTEE, PASSAGE, OF, BILL,...   \n",
       "24Jul2007Cardin165.txt  [[SENATOR, CARDIN, CALLS, ON, ATTORNEY, GENERA...   \n",
       "24Apr2007Cardin234.txt  [[CARDIN, AND, MIKULKSI, INTRODUCE, BILL, TO, ...   \n",
       "20Jun2007Cardin193.txt  [[CARDIN, EXPRESSES, DISAPPOINTMENT, ON, FAILU...   \n",
       "10Sep2007Cardin116.txt  [[CARDIN, DEMANDS, CORRECTION, TO, HELP, SENIO...   \n",
       "12Jul2007Cardin174.txt  [[CARDIN, MIKULSKI, ANNOUNCE, 144, 200, FOR, C...   \n",
       "02Mar2007Cardin276.txt  [[CARDIN, MIKULSKI, ANNOUNCE, 365, 744, IN, FE...   \n",
       "06Dec2007Cardin21.txt   [[CARDIN, MIKULSKI, ANNOUNCE, FEDERAL, FUNDING...   \n",
       "\n",
       "                                                         normalized_sents  \n",
       "21Jun2007Cardin189.txt  [[cardin, deeply, disappointed, president, dec...  \n",
       "07Nov2007Cardin47.txt   [[senate, committee, overwhelmingly, approves,...  \n",
       "06Sep2007Cardin126.txt  [[cardin, hails, committee, passage, bill, end...  \n",
       "24Jul2007Cardin165.txt  [[senator, cardin, calls, attorney, general, g...  \n",
       "24Apr2007Cardin234.txt  [[cardin, mikulksi, introduce, bill, give, sta...  \n",
       "20Jun2007Cardin193.txt  [[cardin, expresses, disappointment, failure, ...  \n",
       "10Sep2007Cardin116.txt  [[cardin, demands, correction, help, seniors, ...  \n",
       "12Jul2007Cardin174.txt  [[cardin, mikulski, announce, cumberland, vall...  \n",
       "02Mar2007Cardin276.txt  [[cardin, mikulski, announce, federal, funding...  \n",
       "06Dec2007Cardin21.txt   [[cardin, mikulski, announce, federal, funding...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardinDF = lucem_illud.loadTextDirectory('../data/grimmerPressReleases_extra/Cardin')\n",
    "cardinDF['category'] = 'Cardin'\n",
    "\n",
    "cardinDF['tokenized_sents'] = cardinDF['text'].apply(lambda x: \n",
    "                                                               [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "cardinDF['normalized_sents'] = cardinDF['tokenized_sents'].apply(lambda x: \n",
    "                                                                           [lucem_illud.normalizeTokens(s, \n",
    "                                                                                           stopwordLst = lucem_illud.stop_words_basic, \n",
    "                                                                                           stemmer = None) \n",
    "                                                                            for s in x])\n",
    "\n",
    "cardinDF[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now merge Cardin's releases with the rest. This can update all the weights in *w2v* model, so be careful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is using a newer API so we have to tell it a little bit more for it to work right\n",
    "senReleasesW2V.build_vocab(cardinDF['normalized_sents'].sum(), update=True)\n",
    "senReleasesW2V.train(cardinDF['normalized_sents'].sum(), \n",
    "                     total_examples=senReleasesW2V.corpus_count, \n",
    "                     epochs=senReleasesW2V.iter)\n",
    "senReleasesW2V.save(\"senpressreleasesWORD2Vec_new.mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model is updated by this, we need to load our old copy to do a comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "senReleasesW2V_old = gensim.models.word2vec.Word2Vec.load('senpressreleasesWORD2Vec.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 100 dimesional vector:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.03221421,  0.10700563, -0.17510892,  1.1004535 ,  0.4304878 ,\n",
       "       -1.5442505 , -2.289932  , -0.22287789,  2.6134236 , -1.8283719 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"A {} dimesional vector:\".format(senReleasesW2V['president'].shape[0]))\n",
    "senReleasesW2V['president'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 100 dimesional vector:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.80673444, -1.6036301 , -0.20755723,  0.1727035 ,  0.61403614,\n",
       "        1.0187216 , -1.9063717 ,  0.6486584 ,  1.2541502 , -0.6843093 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"A {} dimesional vector:\".format(senReleasesW2V_old['president'].shape[0]))\n",
    "senReleasesW2V_old['president'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.83894867, -1.7106357 , -0.03244831, -0.92775   ,  0.18354833,\n",
       "        2.562972  ,  0.3835603 ,  0.87153625, -1.3592734 ,  1.1440625 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(senReleasesW2V_old['president'] - senReleasesW2V['president'])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the vector for 'president' has changed a little bit and the word cloud should also be a bit different too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordsSubMatrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-a2d14a149044>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpcaWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordsSubMatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mreducedPCA_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpcaWords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordsSubMatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#T-SNE is theoretically better, but you should experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtsneWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanifold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_exaggeration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreducedPCA_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wordsSubMatrix' is not defined"
     ]
    }
   ],
   "source": [
    "pcaWords = sklearn.decomposition.PCA(n_components = 50).fit(wordsSubMatrix)\n",
    "reducedPCA_data = pcaWords.transform(wordsSubMatrix)\n",
    "#T-SNE is theoretically better, but you should experiment\n",
    "tsneWords = sklearn.manifold.TSNE(n_components = 2, early_exaggeration = 25).fit_transform(reducedPCA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "plt.scatter(tsneWords[:, 0], tsneWords[:, 1], alpha = 0)#Making the points invisible \n",
    "for i, word in enumerate(targetWords):\n",
    "    ax.annotate(word, \n",
    "                (tsneWords[:, 0][i],tsneWords[:, 1][i]), \n",
    "                size =  20 * (numWords - i) / numWords, \n",
    "                alpha = .8 * (numWords - i) / numWords + .2)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing to consider in training is how we know that our parameters for the model are correct. We can do this by looking at the training loss of the model. Let's tart by training a new model, but this time we will expose most of the options and train it one epoch at a time. Look [here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec) for more detail: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senReleasesW2V_loss = gensim.models.word2vec.Word2Vec(size = 100, #dimensions\n",
    "                                                      alpha=0.025,\n",
    "                                                      window=5,\n",
    "                                                      min_count=5,\n",
    "                                                      hs=0,  #hierarchical softmax toggle\n",
    "                                                      compute_loss = True,\n",
    "                                                     )\n",
    "senReleasesW2V_loss.build_vocab(senReleasesDF['normalized_sents'].sum())\n",
    "senReleasesW2V_loss.train(senReleasesDF['normalized_sents'].sum(), \n",
    "                     total_examples=senReleasesW2V.corpus_count, \n",
    "                     epochs=1, #This the running_training_loss is a total so we have to do 1 epoch at a time\n",
    "                    )\n",
    "#Using a list so we can capture every epoch\n",
    "losses = [senReleasesW2V_loss.running_training_loss]\n",
    "losses[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the training loss and can optimize training to minimize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(19):\n",
    "    senReleasesW2V_loss.train(senReleasesDF['normalized_sents'].sum(), \n",
    "                     total_examples=senReleasesW2V.corpus_count, \n",
    "                     epochs=1,\n",
    "                             )\n",
    "    losses.append(senReleasesW2V_loss.running_training_loss)\n",
    "    print(\"Done epoch {}\".format(i + 2), end = '\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the loss vs epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossesDF = pandas.DataFrame({'loss' : losses, 'epoch' : range(len(losses))})\n",
    "lossesDF.plot(y = 'loss', x = 'epoch', logy=False, figsize=(15, 7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the loss is almost monotonic, but that the rate decreases as epoch increases. Since we are testing on our training data monotonicity is a common result and we must try to avoid over fitting. A simple way to do this is to stop training when there is significant change in the rate of decrease. In this run, that looks to be approximately 8 or 9. If we were to do another analysis, we might use an `iter=9` instead of the default 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more culturally interesting is how many dimensions are required to achieve an optimal embedding. The use of words in complex ways and contradictory contexts will require more dimensions to represent them with integrity. For example, if one word, $w_a$, is \"nearby\" $w_b$, but $w_b$ is not near the other words beside $w_a$, then a new dimension will be required for the two words to be uniquely together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_dims=[]\n",
    "\n",
    "for d in [50,100,150,200,250,300,350,400,450,500, 550, 600, 650, 700, 750]:\n",
    "    senReleasesW2V_loss_dims = gensim.models.word2vec.Word2Vec(size = d, #dimensions\n",
    "                                                      alpha=0.025,\n",
    "                                                      window=5,\n",
    "                                                      min_count=5,\n",
    "                                                      hs=0,  #hierarchical softmax toggle\n",
    "                                                      compute_loss = True,\n",
    "                                                     )\n",
    "    senReleasesW2V_loss_dims.build_vocab(senReleasesDF['normalized_sents'].sum())\n",
    "    senReleasesW2V_loss_dims.train(senReleasesDF['normalized_sents'].sum(), \n",
    "                     total_examples=senReleasesW2V.corpus_count, \n",
    "                     epochs=7, #This the running_training_loss is a total so we have to do 1 epoch at a time\n",
    "                    )\n",
    "    senReleasesW2V_loss_dims.train(senReleasesDF['normalized_sents'].sum(), \n",
    "                     total_examples=senReleasesW2V.corpus_count, \n",
    "                     epochs=1, #This the running_training_loss is a total so we have to do 1 epoch at a time\n",
    "                    )\n",
    "    \n",
    "    losses_dims.append(senReleasesW2V_loss_dims.running_training_loss/(10+d*10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_dimsDF = pandas.DataFrame({'loss' : losses_dims, 'dimensions' : [50,100,150,200,250,300,350,400,450,500,550,600,650,700,750]})\n",
    "losses_dimsDF.plot(y = 'loss', x = 'dimensions', logy=False, figsize=(15, 7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a work in progress for Gensim, but its clear that most of the word distance variation is captured by 300 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=gensim.models.Word2Vec.load('../data/1992embeddings_hs_new3.sg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analogy\n",
    "\n",
    "King+man-Queen? A few examples based on a corpus of Chinese news. \n",
    "\n",
    "First, location analogy: **province -> capital**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = model.most_similar(positive=[u'长沙',u'陕西'], negative=[u'湖南']) # Changsha + Shaanxi - Hunan\n",
    "for m in mm:\n",
    "    print(m[0],m[1])\n",
    "    print(\"Xi'an\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = model.most_similar(positive=[u'广州',u'湖北'], negative=[u'广东']) # Guangzhou + Hubei - Guangdong\n",
    "for m in mm:\n",
    "    print(m[0],m[1])\n",
    "    print(\"Wuhan\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, location analogy: **country -> capital**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = model.most_similar(positive=[u'东京',u'美国'], negative=[u'日本']) # Tokyo + US - Japan\n",
    "for m in mm:\n",
    "    print(m[0],m[1])\n",
    "    print(\"(Washington DC)\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = u'社会主义'  #socialism\n",
    "ss = model.similar_by_word(word,topn=10)\n",
    "print(\"the most similar words to \" + word + \" is: \")\n",
    "for s in ss:\n",
    "    print(s[0])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = u'玉米'  # corn\n",
    "ss = model.similar_by_word(word,topn=10)\n",
    "print(\"the most similar words to \" + word + \" is: \")\n",
    "for s in ss:\n",
    "    print(s[0])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that build a word2vec model with your corpus. Interrogate word relationships in the resulting space, including estimating 90% confidence intervals for specific word cosine distances of interest. Plot a subset of your words. What do these word relationships reveal about the *social* and *cultural game* underlying your corpus? What was surprising--what violated your prior understanding of the corpus? What was expected--what confirmed your knowledge about this domain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This time I'll be using a corpus that's very similar to the last one, but with a twist\n",
    "# I'll be using the same OECD trade policy papers (n = 117)\n",
    "# but this time I'll be using the full papers instead of just the abstracts\n",
    "\n",
    "#So first I have to convert the papers, which are pdfs currently, into text files\n",
    "\n",
    "#Stuff for pdfs\n",
    "#Install as `pdfminer2`\n",
    "import pdfminer.pdfinterp\n",
    "import pdfminer.converter\n",
    "import pdfminer.layout\n",
    "import pdfminer.pdfpage\n",
    "import io #for making http requests look like files\n",
    "\n",
    "def readPDF(pdfFile):\n",
    "    #Based on code from http://stackoverflow.com/a/20905381/4955164\n",
    "    #Using utf-8, if there are a bunch of random symbols try changing this\n",
    "    codec = 'utf-8'\n",
    "    rsrcmgr = pdfminer.pdfinterp.PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    layoutParams = pdfminer.layout.LAParams()\n",
    "    device = pdfminer.converter.TextConverter(rsrcmgr, retstr, laparams = layoutParams, codec = codec)\n",
    "    #We need a device and an interpreter\n",
    "    interpreter = pdfminer.pdfinterp.PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = ''\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "    for page in pdfminer.pdfpage.PDFPage.get_pages(pdfFile, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    device.close()\n",
    "    returnedString = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return returnedString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "pdf_file = open('../4-Word-Embedding/papers/2010 - bilateral and regional trade agreements and technical barriers to trade - an african perspective.pdf', 'rb')\n",
    "pdf_file1 = open('../4-Word-Embedding/papers/2010 - export restrictions on strategic raw materials and their impact on trade.pdf', 'rb')\n",
    "pdf_file2 = open('../4-Word-Embedding/papers/2010 - increasing the impact of trade expansion on growth - lessons from trade reforms for the design of aid for trade.pdf', 'rb')\n",
    "#exampleDF = pandas.DataFrame()\n",
    "# {readPDF(pdf_file)}\n",
    "examplePaper = readPDF('../4')\n",
    "print(type(examplePaper))\n",
    "#exampleBigDF = pandas.DataFrame(data =[''], columns = ['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a title 2</td>\n",
       "      <td>Please cite this paper as:\\n\\nMeyer, N. et al....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category                                               text\n",
       "0        NaN                                                   \n",
       "0  a title 2  Please cite this paper as:\\n\\nMeyer, N. et al...."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exampleBigDF = pandas.DataFrame(columns = ['text','category'])\n",
    "exampleDF = pandas.DataFrame(data =[''], columns = ['text'])\n",
    "exampleDF['text'][0] = examplePaper\n",
    "exampleDF['category'] = \"a title 2\"\n",
    "\n",
    "exampleBigDF = exampleBigDF.append(exampleDF, ignore_index = False)\n",
    "exampleBigDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "PSTypeError",
     "evalue": "Literal required: /b'begin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPSTypeError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d9bc43132af4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpdf_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdfDir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpapDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mpapDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadPDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mpapDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpapersDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpapersDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpapDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-6c17bfd7d758>\u001b[0m in \u001b[0;36mreadPDF\u001b[0;34m(pdfFile)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mpagenos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdfpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPDFPage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdfFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpagenos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxpages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxpages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcaching\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_extractable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mreturnedString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pdfminer/pdfinterp.py\u001b[0m in \u001b[0;36mprocess_page\u001b[0;34m(self, page)\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0mctm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_contents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pdfminer/pdfinterp.py\u001b[0m in \u001b[0;36mrender_contents\u001b[0;34m(self, resources, streams, ctm)\u001b[0m\n\u001b[1;32m    842\u001b[0m         logging.info('render_contents: resources=%r, streams=%r, ctm=%r',\n\u001b[1;32m    843\u001b[0m                      resources, streams, ctm)\n\u001b[0;32m--> 844\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstreams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pdfminer/pdfinterp.py\u001b[0m in \u001b[0;36minit_resources\u001b[0;34m(self, resources)\u001b[0m\n\u001b[1;32m    348\u001b[0m                         \u001b[0mobjid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                     \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfontmap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfontid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsrcmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_font\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ColorSpace'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcsid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pdfminer/pdfinterp.py\u001b[0m in \u001b[0;36mget_font\u001b[0;34m(self, objid, spec)\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                         \u001b[0msubspec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mfont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_font\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mSTRICT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pdfminer/pdfinterp.py\u001b[0m in \u001b[0;36mget_font\u001b[0;34m(self, objid, spec)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0msubtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'CIDFontType0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CIDFontType2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;31m# CID Font\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 \u001b[0mfont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPDFCIDFont\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0msubtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Type0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;31m# Type0 Font\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pdfminer/pdffont.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rsrcmgr, spec, STRICT)\u001b[0m\n\u001b[1;32m    670\u001b[0m             \u001b[0mstrm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ToUnicode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0municode_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFileUnicodeMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m             \u001b[0mCMapParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0municode_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcidcoding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Adobe-Identity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Adobe-UCS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mttf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pdfminer/cmapdb.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnextobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPSEOF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pdfminer/psparser.py\u001b[0m in \u001b[0;36mnextobject\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPSKeyword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'do_keyword: pos=%r, token=%r, stack=%r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurstack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_keyword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unknown token: pos=%r, token=%r, stack=%r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurstack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pdfminer/cmapdb.py\u001b[0m in \u001b[0;36mdo_keyword\u001b[0;34m(self, pos, token)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliteral_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mPSSyntaxError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pdfminer/psparser.py\u001b[0m in \u001b[0;36mliteral_name\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPSLiteral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mSTRICT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPSTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Literal required: %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPSTypeError\u001b[0m: Literal required: /b'begin'"
     ]
    }
   ],
   "source": [
    "pdfDir = '../4-Word-Embedding/papers'\n",
    "\n",
    "papersDF = pandas.DataFrame(columns = ['text','category'])\n",
    "\n",
    "\n",
    "for filename in os.listdir(pdfDir):\n",
    "    pdf_file = open(pdfDir + '/' + filename, 'rb')\n",
    "    papDF = pandas.DataFrame(data =[''], columns = ['text'])\n",
    "    papDF['text'][0] = readPDF(pdf_file)\n",
    "    papDF['category'] = filename\n",
    "    papersDF = papersDF.append(papDF, ignore_index = False)\n",
    "\n",
    "#for papersDir in (file for file in os.scandir(pdfDir) if not file.name.startswith('.') and file.is_file()):\n",
    "#    print(\"howdy\")\n",
    "#    papDF = pandas.DataFrame(data =[''], columns = ['text'])\n",
    "#    papDF['text'][0] = 'title'#readPDF(papersDir.path)\n",
    "#    papDF['category'] = papersDir.name\n",
    "#    papersDF = papersDF.append(papDF, ignore_index = False)\n",
    "\n",
    "papersDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please cite this paper as:\\n\\nBenz, S., A. Kha...</td>\n",
       "      <td>2017 - services and performance of the indian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please cite this paper as:\\n\\nStone, S., P. So...</td>\n",
       "      <td>2013 - trade and labour market adjustment.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please cite this paper as:\\n\\nKowalski, P. et ...</td>\n",
       "      <td>2013 - state-owned enterprises - trade effects...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please cite this paper as:\\n\\nKim, J. (2010), ...</td>\n",
       "      <td>2010 - recent trends in export restrictions.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please cite this paper as:\\n\\nOECD (2016), “Gl...</td>\n",
       "      <td>2016 - global value chains and trade in value-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please cite this paper as:\\n\\nMoïsé, E. (2013)...</td>\n",
       "      <td>2013 - the costs and challenges of implementin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Please cite this paper as:\\n\\nBenz, S., A. Kha...   \n",
       "0  Please cite this paper as:\\n\\nStone, S., P. So...   \n",
       "0  Please cite this paper as:\\n\\nKowalski, P. et ...   \n",
       "0  Please cite this paper as:\\n\\nKim, J. (2010), ...   \n",
       "0  Please cite this paper as:\\n\\nOECD (2016), “Gl...   \n",
       "0  Please cite this paper as:\\n\\nMoïsé, E. (2013)...   \n",
       "\n",
       "                                            category  \n",
       "0  2017 - services and performance of the indian ...  \n",
       "0      2013 - trade and labour market adjustment.pdf  \n",
       "0  2013 - state-owned enterprises - trade effects...  \n",
       "0    2010 - recent trends in export restrictions.pdf  \n",
       "0  2016 - global value chains and trade in value-...  \n",
       "0  2013 - the costs and challenges of implementin...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I couldn't figure out how to extract the text content from my pdfs, so this is as far as I got"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Instead of just looking at just how words embed within in the space, we can look at how the different documents relate to each other within the space. First lets load our data--abstracts of most U.S. physics papers from the 1950s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsDF = pandas.read_csv('../data/APSabstracts1950s.csv', index_col = 0)\n",
    "apsDF[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load these as documents into Word2Vec, but first we need to normalize and pick some tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = ['photomagnetoelectric', 'quantum', 'boltzmann', 'proton', 'positron', 'feynman', 'classical', 'relativity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apsDF['tokenized_words'] = apsDF['abstract'].apply(lambda x: nltk.word_tokenize(x))\n",
    "apsDF['normalized_words'] = apsDF['tokenized_words'].apply(lambda x: lucem_illud.normalizeTokens(x, stopwordLst = lucem_illud.stop_words_basic, stemmer = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taggedDocs = []\n",
    "for index, row in apsDF.iterrows():\n",
    "    #Just doing a simple keyword assignment\n",
    "    docKeywords = [s for s in keywords if s in row['normalized_words']]\n",
    "    docKeywords.append(row['copyrightYear'])\n",
    "    docKeywords.append(row['doi']) #This lets us extract individual documnets since doi's are unique\n",
    "    taggedDocs.append(gensim.models.doc2vec.LabeledSentence(words = row['normalized_words'], tags = docKeywords))\n",
    "apsDF['TaggedAbstracts'] = taggedDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a Doc2Vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apsD2V = gensim.models.doc2vec.Doc2Vec(apsDF['TaggedAbstracts'], size = 100) #Limiting to 100 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get vectors for the tags/documents, just as we did with words. Documents are actually the centroids (high dimensional average points) of their words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apsD2V.docvecs[1952]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words can still be accessed in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V['atom']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still use the ``most_similar`` command to perform simple semantic equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V.most_similar(positive = ['atom','electrons'], negative = ['electron'], topn = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting. **Electron** is to **electrons** as **atom** is to **atoms**. Another way to understand this, developed below is: **electrons - electron** induces a singular to plural dimension, so when we subtract **electron** from **atom** and add **electrons**, we get **atoms**! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V.most_similar(positive = ['einstein','law'], negative = ['equation'], topn = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words **Einstein** minus **equation** plus **law** equals **Meissner**--Walthur Meissner studied mechanical engineering and physics ... and was more likely to produce a \"law\" than a \"equation\", like the Meissner effect, the damping of the magnetic field in superconductors. If we built our word-embedding with a bigger corpus like the entire arXiv, a massive repository of physics preprints, we would see many more such relationships like **gravity - Newton + Einstein = relativity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute all of these *by hand*--explicitly wth vector algebra: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.pairwise.cosine_similarity(apsD2V['electron'].reshape(1,-1), apsD2V['positron'].reshape(1,-1))\n",
    "#We reorient the vectors with .reshape(1, -1) so that they can be computed without a warning in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the doc2vec model, the documents have vectors just as the words do, so that we can compare documents with each other and also with words (similar to how a search engine locates a webpage with a query). First, we will calculate the distance between a word and documents in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V.docvecs.most_similar([ apsD2V['electron'] ], topn=5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we search for the first of these on the web (these are doi codes), we find the following...a pretty good match:\n",
    "<img src='../data/PhysRev.98.875.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go the other way around and find words most similar to this document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V.most_similar( [ apsD2V.docvecs['10.1103/PhysRev.98.875'] ], topn=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even look for documents most like a query composed of multiple words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V.docvecs.most_similar([ apsD2V['electron']+apsD2V['positron']+apsD2V['neutron']], topn=5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot some words and documents against one another with a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heatmapMatrix = []\n",
    "for tagOuter in keywords:\n",
    "    column = []\n",
    "    tagVec = apsD2V.docvecs[tagOuter].reshape(1, -1)\n",
    "    for tagInner in keywords:\n",
    "        column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, apsD2V.docvecs[tagInner].reshape(1, -1))[0][0])\n",
    "    heatmapMatrix.append(column)\n",
    "heatmapMatrix = np.array(heatmapMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hmap = ax.pcolor(heatmapMatrix, cmap='terrain')\n",
    "cbar = plt.colorbar(hmap)\n",
    "\n",
    "cbar.set_label('cosine similarity', rotation=270)\n",
    "a = ax.set_xticks(np.arange(heatmapMatrix.shape[1]) + 0.5, minor=False)\n",
    "a = ax.set_yticks(np.arange(heatmapMatrix.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "a = ax.set_xticklabels(keywords, minor=False, rotation=270)\n",
    "a = ax.set_yticklabels(keywords, minor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a heatmap of similarities between the first ten documents in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targetDocs = apsDF['doi'][:10]\n",
    "\n",
    "heatmapMatrixD = []\n",
    "\n",
    "for tagOuter in targetDocs:\n",
    "    column = []\n",
    "    tagVec = apsD2V.docvecs[tagOuter].reshape(1, -1)\n",
    "    for tagInner in targetDocs:\n",
    "        column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, apsD2V.docvecs[tagInner].reshape(1, -1))[0][0])\n",
    "    heatmapMatrixD.append(column)\n",
    "heatmapMatrixD = np.array(heatmapMatrixD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hmap = ax.pcolor(heatmapMatrixD, cmap='terrain')\n",
    "cbar = plt.colorbar(hmap)\n",
    "\n",
    "cbar.set_label('cosine similarity', rotation=270)\n",
    "a = ax.set_xticks(np.arange(heatmapMatrixD.shape[1]) + 0.5, minor=False)\n",
    "a = ax.set_yticks(np.arange(heatmapMatrixD.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "a = ax.set_xticklabels(targetDocs, minor=False, rotation=270)\n",
    "a = ax.set_yticklabels(targetDocs, minor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a heatmap of similarities between the first ten documents and our keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heatmapMatrixC = []\n",
    "\n",
    "for tagOuter in targetDocs:\n",
    "    column = []\n",
    "    tagVec = apsD2V.docvecs[tagOuter].reshape(1, -1)\n",
    "    for tagInner in keywords:\n",
    "        column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, apsD2V.docvecs[tagInner].reshape(1, -1))[0][0])\n",
    "    heatmapMatrixC.append(column)\n",
    "heatmapMatrixC = np.array(heatmapMatrixC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hmap = ax.pcolor(heatmapMatrixC, cmap='terrain')\n",
    "cbar = plt.colorbar(hmap)\n",
    "\n",
    "cbar.set_label('cosine similarity', rotation=270)\n",
    "a = ax.set_xticks(np.arange(heatmapMatrixC.shape[1]) + 0.5, minor=False)\n",
    "a = ax.set_yticks(np.arange(heatmapMatrixC.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "a = ax.set_xticklabels(keywords, minor=False, rotation=270)\n",
    "a = ax.set_yticklabels(targetDocs, minor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the model in case we would like to use it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apsD2V.save('apsW2V')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can later load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#apsD2V = gensim.models.word2vec.Word2Vec.load('data/apsW2V')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that build a doc2vec model with your corpus. Interrogate document and word relationships in the resulting space. Construct a heatmap that plots the distances between a subset of your documents against each other, and against a set of informative words. Find distances between *every* document in your corpus and a word or query of interest. What do these doc-doc proximities reveal about your corpus? What do these word-doc proximities highlight? Demonstrate and document one reasonable way to select a defensible subset of query-relevant documents for subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also project word vectors to an arbitray semantic dimension. To demonstrate this possibility, let's first load a model trained with New York Times news articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nytimes_model = gensim.models.KeyedVectors.load_word2vec_format('../data/nytimes_cbow.reduced.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can visualize with dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words to create dimensions\n",
    "tnytTargetWords = ['man','him','he', 'woman', 'her', 'she', 'black','blacks','African', 'white', 'whites', 'Caucasian', 'rich', 'richer', 'richest', 'expensive', 'wealthy', 'poor', 'poorer', 'poorest', 'cheap', 'inexpensive']\n",
    "#words we will be mapping\n",
    "tnytTargetWords += [\"doctor\",\"lawyer\",\"plumber\",\"scientist\",\"hairdresser\", \"nanny\",\"carpenter\",\"entrepreneur\",\"musician\",\"writer\", \"banker\",\"poet\",\"nurse\", \"steak\", \"bacon\", \"croissant\", \"cheesecake\", \"salad\", \"cheeseburger\", \"vegetables\", \"beer\", \"wine\", \"pastry\", \"basketball\", \"baseball\", \"boxing\", \"softball\", \"volleyball\", \"tennis\", \"golf\", \"hockey\", \"soccer\"]\n",
    "\n",
    "\n",
    "wordsSubMatrix = []\n",
    "for word in tnytTargetWords:\n",
    "    wordsSubMatrix.append(nytimes_model[word])\n",
    "wordsSubMatrix = np.array(wordsSubMatrix)\n",
    "wordsSubMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcaWordsNYT = sklearn.decomposition.PCA(n_components = 50).fit(wordsSubMatrix)\n",
    "reducedPCA_dataNYT = pcaWordsNYT.transform(wordsSubMatrix)\n",
    "#T-SNE is theoretically better, but you should experiment\n",
    "tsneWordsNYT = sklearn.manifold.TSNE(n_components = 2).fit_transform(reducedPCA_dataNYT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "plt.scatter(tsneWordsNYT[:, 0], tsneWordsNYT[:, 1], alpha = 0) #Making the points invisible\n",
    "for i, word in enumerate(tnytTargetWords):\n",
    "    ax.annotate(word, (tsneWordsNYT[:, 0][i],tsneWordsNYT[:, 1][i]), size =  20 * (len(tnytTargetWords) - i) / len(tnytTargetWords))\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some convenient functions for getting dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(vector):\n",
    "    normalized_vector = vector / np.linalg.norm(vector)\n",
    "    return normalized_vector\n",
    "\n",
    "def dimension(model, positives, negatives):\n",
    "    diff = sum([normalize(model[x]) for x in positives]) - sum([normalize(model[y]) for y in negatives])\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate three dimensions: gender, race, and class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Gender = dimension(nytimes_model, ['man','him','he'], ['woman', 'her', 'she'])\n",
    "Race = dimension(nytimes_model, ['black','blacks','African'], ['white', 'whites', 'Caucasian'])\n",
    "Class = dimension(nytimes_model, ['rich', 'richer', 'richest', 'expensive', 'wealthy'], ['poor', 'poorer', 'poorest', 'cheap', 'inexpensive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Occupations = [\"doctor\",\"lawyer\",\"plumber\",\"scientist\",\"hairdresser\", \"nanny\",\"carpenter\",\"entrepreneur\",\"musician\",\"writer\", \"banker\",\"poet\",\"nurse\"]\n",
    "\n",
    "Foods = [\"steak\", \"bacon\", \"croissant\", \"cheesecake\", \"salad\", \"cheeseburger\", \"vegetables\", \"beer\", \"wine\", \"pastry\"]\n",
    "\n",
    "Sports  = [\"basketball\", \"baseball\", \"boxing\", \"softball\", \"volleyball\", \"tennis\", \"golf\", \"hockey\", \"soccer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to project words in a word list to each of the three dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeDF(model, word_list):\n",
    "    g = []\n",
    "    r = []\n",
    "    c = []\n",
    "    for word in word_list:\n",
    "        g.append(sklearn.metrics.pairwise.cosine_similarity(nytimes_model[word].reshape(1,-1), Gender.reshape(1,-1))[0][0])\n",
    "        r.append(sklearn.metrics.pairwise.cosine_similarity(nytimes_model[word].reshape(1,-1), Race.reshape(1,-1))[0][0])\n",
    "        c.append(sklearn.metrics.pairwise.cosine_similarity(nytimes_model[word].reshape(1,-1), Class.reshape(1,-1))[0][0])\n",
    "    df = pandas.DataFrame({'gender': g, 'race': r, 'class': c}, index = word_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OCCdf = makeDF(nytimes_model, Occupations) \n",
    "Fooddf = makeDF(nytimes_model, Foods)\n",
    "Sportsdf = makeDF(nytimes_model, Sports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some useful functions for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Coloring(Series):\n",
    "    x = Series.values\n",
    "    y = x-x.min()\n",
    "    z = y/y.max()\n",
    "    c = list(plt.cm.rainbow(z))\n",
    "    return c\n",
    "\n",
    "def PlotDimension(ax,df, dim):\n",
    "    ax.set_frame_on(False)\n",
    "    ax.set_title(dim, fontsize = 20)\n",
    "    colors = Coloring(df[dim])\n",
    "    for i, word in enumerate(df.index):\n",
    "        ax.annotate(word, (0, df[dim][i]), color = colors[i], alpha = 0.6, fontsize = 12)\n",
    "    MaxY = df[dim].max()\n",
    "    MinY = df[dim].min()\n",
    "    plt.ylim(MinY,MaxY)\n",
    "    plt.yticks(())\n",
    "    plt.xticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the occupational words in each of the three dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "PlotDimension(ax1, OCCdf, 'gender')\n",
    "ax2 = fig.add_subplot(132)\n",
    "PlotDimension(ax2, OCCdf, 'race')\n",
    "ax3 = fig.add_subplot(133)\n",
    "PlotDimension(ax3, OCCdf, 'class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "PlotDimension(ax1, Fooddf, 'gender')\n",
    "ax2 = fig.add_subplot(132)\n",
    "PlotDimension(ax2, Fooddf, 'race')\n",
    "ax3 = fig.add_subplot(133)\n",
    "PlotDimension(ax3, Fooddf, 'class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "PlotDimension(ax1, Sportsdf, 'gender')\n",
    "ax2 = fig.add_subplot(132)\n",
    "PlotDimension(ax2, Sportsdf, 'race')\n",
    "ax3 = fig.add_subplot(133)\n",
    "PlotDimension(ax3, Sportsdf, 'class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that identify semantic dimensions of interest from your data (e.g., gender: man-woman) and project words onto these dimensions. Plot the array of relevant words along each semantic dimension. Which words are most different. Which dimensions are most different? On which dimension are your words most different? Print three short textual examples from the corpus that illustrate the association you have explored.\n",
    "\n",
    "<span style=\"color:red\">***Stretch***: Project documents from your corpus along a dimension of interest. Sample relevant documents from your corpus with this functionality and explain your rationale? Calculate the cosine of the angle between two dimensions (encoded as vectors) of interest. What does this suggest about the relationship between them within your corpus? \n",
    "\n",
    "<span style=\"color:red\">***Stretch***: Create 90% bootstrap confidence intervals around your word projections onto a given dimension. Which words are *significantly* different on your semantic dimension of interest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
